\documentclass{report} 
\title{Signals And Systems by Alan V. Oppenheim:\\Notes}
\date{Started 17 April 2025}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm}%bold math
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Introduction}
\subsection{Signal Energy and Power}
\textbf{Motivation and definition}\\
In many but not all, applications, the signals considered directly related to physical quantities capturing
power and energy in a physical system. (for instance $v^2/R$ for the power across a resistor)\\
\vspace{1mm}\\
As such it is a common and worthwhile convention to use similar terminology for power and energy for \textit{any} 
continuous-time signal, denoted $x(t)$, or any discrete-time signal $x[n]$. 
In this case, the total energy over the time interval $t_1\leq t\leq t_2$ in a continuous signal $x(t)$ is defined
as
\begin{equation*}
\int^{t_2}_{t_1}|x(t)|^2dt
\end{equation*}
where $|x|$ denotes the magnitude of the (possibly complex) number $x$; see that the time-averaged signal 
can be obtained by dividing by $(t_2-t_1)$. Similarly for a discrete signal $x[n]$ over the interval $n_1\leq n\leq n_2$ the total energy is
\begin{equation*}
\sum^{n_2}_{n=n_1}|x[n]|^2
\end{equation*}
with the average power calculated by dividing by $(n_2-n_1+1)$.\\
\vspace{1mm}\\
It is important to remember that the terms `power' and `energy' are used here \textit{independently} of their 
relation to physical energy (they clearly don't correlate since their units or scalings would differ). Nevertheless
we will find it convenient to use these terms in a general fashion.\\
\vspace{1mm}\\
\textbf{Power and energy over infinite intervals}\\
Considering signals over an infinite time interval, meaning for $-\infty<t<+\infty$ or $-\infty<n<+\infty$. 
Here we define the total energy as the limits of the aforementioned equations increase without bound; in continuous
time,
\begin{equation*}
E_\infty\triangleq\lim_{T\to\infty}\int^T_{-T}|x(t)|^2dt
=\int^{+\infty}_{-\infty}|x(t)|^2dt
\end{equation*}
and in discrete time,
\begin{equation*}
E_\infty\triangleq\lim_{N\to\infty}\sum^{+N}_{n=-N}|x[n]|^2=\sum^{+\infty}_{n=-\infty}|x[n]|^2
\end{equation*}
Note that these expressions may not converge; for instance say $x(t)$ or $x[n]$ equal some nonzero constant for 
all time: such signals have infinite energy, while signals with $E_\infty<\infty$ have finite energy.\\
(next page)\newpage
\noindent\textbf{Cont.}\\
Analagously, we can define the time-averaged power over an infinite interval as
\begin{equation*}
P_\infty\triangleq\lim_{T\to\infty}\frac{1}{2T}\int^T_{-T}|x(t)|^2dt
\end{equation*}
and
\begin{equation*}
P_\infty\triangleq\lim_{N\to\infty}\frac{1}{2N+1}\sum^{+N}_{n=-N}|x[n]|^2
\end{equation*}
In continuous and discrete time respectively. \\
\vspace{1mm}\\
\textbf{Intuition}\\
See that with these definitions, we can identify three classes of signals: first those with finite total energy,
meaning $E_\infty<\infty$. See that such a signal would have zero average power:
\begin{equation*}
P_\infty=\lim_{T\to\infty}\frac{E_\infty}{2T}=0
\end{equation*}
Second would be signals with finite average power $P_\infty$; see from the above expression that for $P_\infty>0$,
this requires that $E_\infty=\infty$.\\
\vspace{1mm}\\
Last would be signals for which neither $P_\infty$ nor $E_\infty$ are finite. An example of this might be $x(t)=t$.\\
\vspace{1mm}\\
\textbf{Note on discrete signals}\\
It is omportant to note that the discrete-time signal $x[n]$ is defined \textit{only} for \textit{integer} 
values of the independent variable.
\newpage

\subsection{Even and Odd signals}
\textbf{Definition}\\
A continuous-time signal is \textit{even} if
\begin{equation*}
x(-t)=x(t)
\end{equation*}
while a discrete-time signal is \textit{even} if
\begin{equation*}
x[-n]=x[n]
\end{equation*}
These signals are referred to as \textit{odd} if
\begin{align*}
x(-t)&=-x(t)\\
x[-n]&=-x[n]
\end{align*}
Note that an odd signal must be 0 at $t=0$ or $n=0$ since the equations require that $x(0)=-x(0)$ and $x[0]=-x[0]$.
\\
\vspace{1mm}\\
\textbf{Decomposition}\\
An important fact is that any signal can be broken into a sum of two signals, where one is even and the other odd.
To see this, consider
\begin{equation*}
\text{Ev}\{x(t)\}=\frac{1}{2}[x(t)+x(-t)]
\end{equation*}
which is referred to as the \textit{even part} of $x(t)$. Similarly, the \textit{odd part} of $x(t)$ is given by
\begin{equation*}
\text{Od}\{x(t)\}=\frac{1}{2}[x(t)-x(-t)]
\end{equation*}
See that $x(t)$ is the sum of the two. Exactly analagous definitions hold in the discrete time case.
\begin{center}
\includegraphics[width=9cm]{a1}
\end{center}
\newpage

\subsection{More on Complex Exponential and\\Sinusoidal Signals}
The continuous-time \textit{complex exponential signal} is of the form
\begin{equation*}
x(t)=Ce^{at}
\end{equation*}
where $C$ and $a$ are, in general, complex numbers. An important class of complex exponentials is obtained by constraining $a$ to be purely imaginary:
\begin{equation*}
x(t)=e^{i\omega_0t}
\end{equation*}
\textbf{Periodicity and harmonic relations (purely imaginary power)}\\
An important property of this signal is that it is periodic; recall that $x(t)$ will be periodic with period $T$ if
\begin{equation*}
e^{i\omega_0t}=e^{i\omega_0(t+T)}
\end{equation*}
this means
\begin{equation*}
e^{i\omega_0(t+T)}=e^{i\omega_0t}e^{i\omega_0T}\implies
e^{i\omega_0T}=1
\end{equation*}
If $w_0=0$ then this is satisfied for any $T$. If $w_0\neq0$, see that the \textit{fundamental period} 
$T_0$ of $x(t)$---that is, the smallest positive value of $T$ for which this holds---is
\begin{equation*}
T_0=\frac{2\pi}{|\omega_0|}
\end{equation*}
(the signals $e^{i\omega_0t}$ and $e^{-i\omega_0t}$ have the same fundamental period)
Naturally, there is a set of exponentials periodic to a common period $T_0$. These are said to be
\textit{harmonically related} complex exponentials; the necessary condition they satisfy is
\begin{equation*}
e^{i\omega T_0}=1
\end{equation*}
which implies that
\begin{equation*}
\omega T_0=2\pi k,\quad k=0,\pm1,\pm2,\ldots
\end{equation*}
if we define 
\begin{equation*}
\omega_0=\frac{2\pi}{T_0}
\end{equation*}
this means that the harmonic frequencies $\omega$ must be integer multiples of $\omega_0$:
\begin{equation*}
\phi_k(t)=e^{ik\omega_0t},\quad k=0,\pm1,\pm2,\ldots
\end{equation*}
For $k=0$, $\phi_k(t)$ is a constant, while for any other value of $k$, $\phi_k(t)$ is periodic with fundamenal 
frequency $|k|\omega_0$ and fundamental period
\begin{equation*}
\frac{2\pi}{|k|\omega_0}=\frac{T_0}{|k|}
\end{equation*}
Note that the $k$th harmonic $\phi_k(t)$ is still periodic with $T_0$; it goes through exactly $|k|$ of its 
fundamental periods during any time interval of length $T_0$. (the term `harmonic' is consistent with its use in
music, where it refers to tones resulting from variations in acoustic pressure at frequencies that are integer
multiples of a fundamental frequency)\\
(next page)\newpage
\noindent\textbf{Discrete case}\\
As in continuous time, an important signal in discrete time is the \textit{complex exponential signal}, defined as
\begin{equation*}
x[n]=C\alpha^n
\end{equation*}
where $C$ and $\alpha$ are, in general, complex numbers. See that this could also be expressed as
\begin{equation*}
x[n]=Ce^{\beta n}
\end{equation*}
where $\alpha=e^\beta$. See that we can constrain $\beta$ to be purely imaginary:
\begin{equation*}
x[n]=e^{i\omega_0n}
\end{equation*}
\textbf{Periodicity properties of Discrete-time complex exponentials}\\
While there are many similarities between continuous and discrete-time signals, there are a number of important
differences. 











\end{document}
