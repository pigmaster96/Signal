\documentclass{report} 
\title{DSP notes 1}
\date{Started April 11th 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm}%bold math
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Convolution}
\subsection{Convolution algorithm}
We consider two algorithms, the first \textit{input} algorithm considers convolution from the 
viewpont of the input signal---how each input sample contributes to
multiple points. The second considers \textit{output} algorithm does the same but for the output signal---how 
an output sample has received information from multiple input samples.\\
\vspace{1mm}\\
The first perspective demonstrates the conceptual understanding of convolution, while the second the mathematics.
Recall the (continuous) definition of the convolution:
\begin{equation*}
(f*g)(t)=\int^{t^+}_{0^-}f(\tau)g(t-\tau)d\tau\quad\text{for }t>0
\end{equation*}
\noindent\textbf{Input side algorithm}\\
Consider a 9 point input signal $x[n]$ passed through a system with a 4 point impulse response $h[n]$.  
The input $x[n]$ is convolved
with $h[n]$ to produce $y[n]$, the system respose to $x[n]$. The convolution amounts to 
\textit{scaling and shifting the impulse response according to each input sample}, and then \textit{adding
them together}.\\
\vspace{1mm}\\
For example, lets say input sample number 4 is $x[4]=1.4$; this contributes an output component of $1.4h(n-4)$, 
essentially
scale the impulse response by 1.4 and shift it four samples to the right. (The input can be represented
as $1.4\delta[n=4]$, which after passing through the system becomes $1.4h[n-4]$ (linearity)).\\
\vspace{1mm}\\
This is repeated for all the input points, so we have 9 output components for 9 input points. We add these 
components together to get the output.\\
(next page)\newpage
\noindent Consider these figures, squares represent the data points that come from the shifted and
scaled input response, and diamonds for the added zeros:
\begin{center}
\includegraphics[width=10cm]{a1}\\
\includegraphics[width=10cm]{a2}\\
\end{center}
The convolution is the resulting sum of the nine scaled and translated impulse responses.\\
\vspace{1mm}\\
To relate this back to the math, intuitively the fourth output point depends on the fourth point of the 
impulse response scaled by the first point of the input, and the third point of the impulse response scaled by 
the second point of the input and so on.\\
(next page)\newpage
\noindent\textbf{Input side program}\\
The program shown here convolves an 81 point input signal held in array $X$ with a 31 point impulse 
response held in array $H$ to produce output array $Y$:
\begin{center}
\includegraphics[width=10cm]{a3}\\
\end{center}
See that the first for loop loops through the \textit{input}, so it looks at each input sample, then scales each 
point of the impulse response (in the second for loop) by that input sample, accordingly incrementing the related
points in the output array.\\
\vspace{1mm}\\
Lastly, since the convolution is commutative, we can swap the input and the impulse response and get
the same solution:
\begin{center}
\includegraphics[width=8.3cm]{a4}\\
\end{center}
(next page)\newpage
\noindent\textbf{Output side algorithm}\\
Now we consider the output side algorithm---how a single output point receives contribution from multiple
input points. First recall the discrete convolution definition:
\begin{equation*}
y[i]=(x*h)[i]=\sum^{M-1}_{j=0}h[j]x[i-j]
\end{equation*}
Where $h$ is an $M$ point signal running from 0 to $M-1$.
(Recall this idea: intuitively the $n$th output point depends on the $n$th point of the 
impulse response scaled by the first point of the input (meaning $h[0]\cdot x[n-0]$), and the $(n-1)$th point of the impulse response scaled by 
the second point of the input (meaning $h[1]\cdot x[n-1]$) and so on.\\
\vspace{1mm}\\
Now see that this can be represented as a `convolution machine', where the inpulse response is 
\textit{flipped left-for-right} and the dot product with the input is taken such that its output aligns 
with the output sample being calculated:
\begin{center}
\includegraphics[width=10cm]{a5}\\
\end{center}
In this instance, $y[6]$ is being calculated from the input samples $x[3],x[4],x[5]$ and $x[6]$. 
To calculate $y[7]$, the 
`convolution machine' moves one sample to the right.\\
(next page)\newpage
\noindent\textbf{Nonexistent samples}\\
See a complcation that arises: when the `convolution machine' is located fully to the left or right, say $y[0]$,
it is trying to receive input from samples that don't exist:
\begin{center}
\includegraphics[width=5cm]{a6}
\includegraphics[width=5cm]{a7}\\
\end{center}
We say that the impulse response is not fully \textit{immersed} in the input signal. 
This can be handled by inventing the nonexistent samples or zero padding.\\
\vspace{1mm}\\
\textbf{Output side program}\\
The output side program loops through each \textit{output sample}:
\begin{center}
\includegraphics[width=10cm]{a8}\\
\end{center}
See that this algorithm essentially just follows the definition of the convolution. Note that lines 210 and 220
preventing indexing into $X$ outside of array bounds. This means that the program only
calculates the samples in the output signal where the impulse response is \textit{fully immersed} in
the input signal. A way around this would be to define the input signal's array from 
$X[-30]$ to $X[110]$, maybe with 30 zeros padded on each side of the true data.
\newpage

\subsection{Delta function}
\textbf{Identity operator for convolution}\\
See
that the delta function is the the \textit{identity} for convolution:
\begin{equation*}
x[n]*\delta[n]=x[n]
\end{equation*}
Think of a system where an impulse on the input produces an identical impulse on the output---all signals are
passed through the system without change; so convolution with the impulse response gives an output which is the
same as the input.\\
\vspace{1mm}\\
\textbf{Scaling and shifting}\\
See that the delta function impulse response can be scaled to \textit{amplify} or \textit{attenuate} it. 
(This comes from the linearity of the integral):
\begin{equation*}
x[n]*k\delta[n]=kx[n]
\end{equation*}
Also see that an impulse response represented by delta function with a \textit{shift} introduces an identical shift between the input and output signals:
\begin{equation*}
x[n]*\delta[n*s]=x[n+s]
\end{equation*}
This could be described as a signal \textit{delay} or \textit{advance}.
\begin{center}
\includegraphics[width=10cm]{a9}\\
\end{center}
\newpage

\subsection{Calculus-like operations}
Convolution can change discrete signals in ways that resemble integration and differentiation. 
The terms `derivative' and `integral' refer to operations on continuous systems; their discrete counterparts are
named differently. For instance the discrete analogue of
the first derivative is called the \textit{first difference}, and that of the integral the \textit{running sum}.\\
\vspace{1mm}\\
Just as with the derivative, the amplitude of each point in the difference signal 
(discrete version of the derivative) is equal to the slope at the corresponding point in the original signal:
\begin{equation*}
y[n]=x[n]-x[n-1]
\end{equation*}
Where $x[n]$ is the original signal and $y[n]$ the first difference.
The running sum is the inverse operation of the first difference. In the perspective of a convolution, see that
the impulse responses are as follows
\begin{center}
\includegraphics[width=10cm]{a10}\\
\end{center}
These impulse responses are simple enough that a full convolution program is usually not needed for implementation;
we can calculate the first difference directly.\\
\vspace{1mm}\\
Note that this is not the only way to define a \textit{discrete derivative}. Another common definition might be
\begin{equation*}
y[n]=(x[n+1]-x[n-1])/2
\end{equation*}
(next page)\newpage
\noindent Illustrated:
\begin{center}
\includegraphics[width=10cm]{a11}\\
\end{center}
Each sample in the running sum is can be calculated by summing all the points in the original sum to the 
\textit{left} of the sample's location (see how this is present in the impulse response):
\begin{equation*}
y[n]=x[n]+y[n-1]
\end{equation*}
The key takeaway here is that these relations are \textit{identical} to convolution using the impulse response 
corresponding to each operation. As mentioned before 
implementation of such operations are simple enough to calculate directly instead of using a convolution program:
\begin{center}
\includegraphics[width=10cm]{a12}\\
\end{center}
\newpage

\subsection{DFT notation}
\textbf{Conventional array notation}\\
In the DFT, lower case letters $x[\quad],y[\quad],\ldots$ represent time domain signals; upper case letters
$X[\quad],Y[\quad],Z[\quad]$ 
represent their frequency domains.\\
\vspace{1mm}\\
The time domain signal contained in a $N$ length array $x[0]$ to $x[127]$ produces two frequency domain 
signals of length $N/2+1$ points, one for each of the sine and cosine \textit{amplitudes}, denoted 
$REX[\quad]$ and $IMX[\quad]$
for cosine and sine respectively. (RE comes from \textit{real} and IM \textit{imaginary}, originating from the
complex DFT. For the real DFT, these essentially mean
cosine and sine wave amplitudes respectively.)\\
\vspace{1mm}\\
The $N/2$ part comes from the nyquist frequency---frequencies above half the sampling rate aren't detectable. 
The $N$ point sample produces $N+2$ cosine and sine frequencies in total;
the additional 2 frequencies come from $IMX[0]$ and $IMX[N/2]$ always having amplitudes of 0. This is illustrated
below.\\
\vspace{1mm}\\
\textbf{Conventional frequency domain notation}\\
Illustrated is a DFT with $N=128$, so 128 samples per second. The time domain signal is contained in a array $x[0]$ 
to $x[127]$. The frequency domain signals are contained
in the arrays $ReX[0]$ to $ReX[64]$ and $ImX[0]$ to $Im[64]$, 65 points each in length:
\begin{center}
\includegraphics[width=10cm]{a13}\\
\end{center}
(next page)\newpage
\noindent\textbf{Conventional frequency domain notation cont.}\\
There












\newpage
\appendix
\chapter{Mathematics (copied from appendices)}

\subsection{Convolution---Definition and properties}
\textbf{Definition}\\
The \textit{convolution} of two functions $f$ and $g$,
denoted by $f*g$, is defined as
\begin{equation*}
(f*g)(t)=\int^{t^+}_{0^-}f(\tau)g(t-\tau)d\tau\quad\text{for }t>0
\end{equation*}
This is a \textit{one-sided} convolution, which is only concerned with functions on the interval $(0^-,\infty)$.\\
\vspace{1mm}\\
\textbf{Properties}\\
\textit{Linearity}; for functions $f_1,f_2,g$ and constants $c_1,c_2$:
\begin{equation*}
(c_1f_1+c_2f_2)*g=c_1(f_1*g)+c_2(f_2*g)
\end{equation*}
This follows from the linearity of integration.\\
\vspace{1mm}\\
\textit{Commutivity}: $f*g=g*f$. This follows from the change of variable $v=t-\tau$; the limits become
\begin{equation*}
\tau=0^-\implies t-\tau=t^+\quad\text{and}\quad\tau=t^+\implies t-\tau=0^-
\end{equation*}
\textit{Associativity}: $f*(g*h)=(f*g)*h$. Showing this amounts to changing the order of integration. 
First consider the discrete case:
\begin{align*}
((f*g)*h)(n)&=\sum^n_{k=0}(f*g)(k)h(n-k)\\
&=\sum^n_{k=0}\left(\sum^k_{i=0}f(l)g(k-l)\right)h(n-k)\\
&=\sum_{0\leq l\leq k\leq n}f(l)g(k-l)h(n-k)\\
&=\sum^n_{l=0}\sum^n_{k=l}f(l)g(k-l)h(n-k)\\
&=\sum^n_{l=0}f(l)\left(\sum^{n-1}_{k=0}g(k)h(n-k-l)\right)\\
&=\sum^n_{l=0}f(l)(g*h)(n-l)\\
&=(f*(g*h))(n)
\end{align*}
(next page)\newpage
\noindent\textbf{Properties cont.}\\
The continuous case is analogously:
\begin{align*}
((f*g)*h)(t)&=\int^t_0(f*g)(s)h(t-s)ds\\
&=\int^t_{s=0}\left(\int^s_{u=0}f(u)g(s-u)du\right)h(t-s)ds\\
&=\iint_{0\leq u\leq s\leq t} f(u)g(s-u)h(t-s)du\,ds\\
&=\int^t_{u=0}f(u)\left(\int^{t-u}_{s=0}g(s)h(t-u-s)ds\right)du\\
&=\int^t_{u=0}f(u)(g*h)(t-u)du\\
&=(f*(g*h))(t)
\end{align*}
\textbf{Delta functions}\\
See that
\begin{equation*}
(\delta*f)(t)=f(t)\quad\text{and}\quad(\delta(t-a)*f)(t)=f(t-a)
\end{equation*}
These can be shown via direct computation. Recall that for $b>0$
\begin{equation*}
\int^b_{0^-}\delta(\tau)f(\tau)d\tau=f(0)
\end{equation*}
So it follows that for $t\geq0$
\begin{align*}
(\delta*f)(t)&=\int^{t^+}_{0^-}\delta(\tau)f(t-\tau)d\tau=
f(t-0)=f(t)\\
(\delta(t-a)*f)(t)&=\int^{t^+}_{0^-}\delta(\tau-a)f(t-\tau)d\tau=f(t-a)
\end{align*}
(for the second statement see that the delta function only has magnitude for $\tau=a$) 
\newpage

\subsection{Green's Formula}
\textbf{Definition}\\
Suppose we have the linear time invariant system with rest initial conditions:
\begin{equation*}
p(D)y=f(t),\quad y(t)=0\text{ for }t<0
\end{equation*}
Suppose that $w(t)$ is the unit impulse response (also called the \textit{weight} function) for the above system. 
That is, $w(t)$ satisfies $p(D)w=\delta(t)$,
with rest initial conditions. \textit{Green's formula} states that for any input $f(t)$ the solution to that
system is given by
\begin{equation*}
y(t)=(f*w)(t)=\int^{t^+}_{0^-}f(\tau)w(t-\tau)d\tau
\end{equation*}
This means we can find the response to \textit{any} input once we know the unit impulse response. It is also
an integral, which can be computed numerically if necessary.\\
\textbf{Intuition}\\
Recall that linear time invariance means 
\begin{equation*}
y(t)\text{ solves }p(D)y=f(t)\implies y(t-a)\text{ solves }p(D)y=f(t-a)
\end{equation*}
(If $y(t)$ is the response to input $f(t)$ then $y(t-a)$ is the response to input $f(t-a)$.) 
First consider partitioning time into intervals of width $\Delta t$; so $t_0=0,t_1=\Delta t,t_2=2\Delta t$, etc.:
\begin{center}
\includegraphics[width=10cm]{56}\\
\end{center}
Next we decompose the input signal $f(t)$ into packets over each interval. The $k$th signal packet, $f_k(t)$
coincides with $f(t)$ between $t_k$ and $t_{k+1}$ and is 0 elsewhere:
\begin{equation*}
f_k(t)=\begin{cases}
f(t)&\text{for }t_k<t<t_{k+1}\\
0&\text{elsewhere}
\end{cases}
\end{equation*}
\begin{center}
\includegraphics[width=10cm]{57}\\
\end{center}
(next page)\newpage
\noindent\textbf{Intuition cont.}\\
Naturally for $t>0$ we have $f(t)$ as the sum of the packets
\begin{equation*}
f(t)=f_0(t)+f_1(t)+\ldots+f_k(t)+\ldots
\end{equation*}
Given that a single packet $f_k(t)$ is concentrated entirely in the small neighbourhood of $t_k$, they can be
approximated as an impulse with the same size as the area under $f_k(t)$, we also approximate the 
area as a rectangle, (like a riemann sum) so 
\begin{equation*}
f_k(t)\approx(f(t_k)\Delta t)\delta(t-t_k)
\end{equation*}
The weight function $w(t)$ is the response to $\delta(t)$. So by linear time invariance the response to $f_k(t)$ is
\begin{equation*}
y_k(t)\approx(f(t_k)\Delta t)w(t-t_k)
\end{equation*}
Say we want to find the response at a fixed time $T$, we
know $f$ (input) is the sum of $f_k$ so by \textit{superposition}
we have $y$ (output) as the sum of $y_k$. At time $T$:
\begin{align*}
y(T)&=y_0(T)+y_1(T)+\ldots\\
&\approx\left(f(t_0)w(T-t_0)+f(t_1)w(T-t_1)+\ldots\right)\Delta t
\end{align*}
We can ignore all the terms where $t_k>T$ as $T-t_k<0$ so $\delta(T-t_k)=0$ and $w(T-t_k)=0$. So
if $n$ is the last index where $t_k<T$ we have
\begin{equation*}
y(T)\approx\left(f(t_0)w(T-t_0)+f(t_1)w(T-t_1)+\ldots+
f(t_n)w(T-t_n)\right)\Delta t
\end{equation*}
This is a riemann sum; as $\Delta t\to0$ it tends to the integral
\begin{equation*}
y(T)=\int^T_0f(t)w(T-t)dt
\end{equation*}
which is the convolution $(y*w)(T)$---the system response is the convolution of the input with the impulse response.




\end{document}
